\<Puppet Capabilities\>
==========================
This document is two things:  A postulate about a step toward application (rather than node) management, and a longer description of how I came to believe that postulate.  The postulate is multipart:

* Puppet's RAL needs to be extended to include the concept of a 'capability', such that one resource can provide a capability and another resource can require that capability
* This 'capability' definition must include sufficient information for the requiring resource to test whether the capability is available and functional - e.g., if the capability is a database service, then we must include sufficient information to connect to that service and confirm it is functional
* Testing this 'capability' is functionally equivalent to monitoring it, and if everything in a Puppet network can monitor all required capabilities, then we have essentially achieved a self-monitoring infrastructure (albeit without data aggregation or alerting); we could, for instance, ping the network and ask everyone to run a quick check on everything they rely on, without having a central polling server

How I came to believe this
==========================
My vision for how we will accomplish application management can be decomposed into two major pieces:  The central piece responsible for compiling the Infrastructure graph, which is all of the applications in the infrastructure and their composite services, and the per-node piece, which is any given node's piece of that infrastructure graph.  My prototyping has primarily focused on the central piece, but having not made progress recently, I decided to refocus on the individual node piece.

One of the questions we've been wrestling with is whether the agent needs to be able to speak directly to the required service (e.g., does the agent on a web server need to be able to talk to a required database?), so as an exercise I began with the assumption that it did (which is the simplest case).  In that model, every resource essentially needs to be able to speak every protocol (e.g., sql, http) and test every capability.  Once this requirement became clear, it was obvious that there couldn't be a general requirement for any resource to do all of these connection types.  At the same time, I realized that monitoring applications have been able to categorize these kinds of capabilities into a relatively small (e.g., 2 to 3 digits) finite list of types of things they need to monitor.

This link between the dependencies on capabilities and the ability to break these capabilities down into monitoring-like categories led me to conclude that (if all of the previous theories are true) we could and should extend the model of a resource to include, along with the existing Type and Provider, the concept of a Capability, which would also have various drivers (e.g., SQL, HTTP).  Some of these capabilities would be network-oriented, i.e., all of the network protocols, but some would also be inherently local, such as disk space and user access.

If we extended the Resource model to include Capabilities, and our configurations defined all of the capability dependencies, then we have an infrastructure that knows how to tell if its dependencies are met in a functional way, not just an ordering way.  I.e., a web service can tell if its needed database service is working and accessible, rather than just if it's been configured.

Things this doesn't address
===========================
I've made no attempt to discuss how the two ends of a dependency know about each other - that is, if a service provides a database capability, I'm not yet saying how its configuration information gets to the requiring web service.  I expect that some part of this needs to be solved in the (currently theoretical) central tool - it's the only tool capable of managing configurations on multiple nodes, and thus information passing between multiple nodes must be at least shepherded by this application.

There is no discussion of ordering across multiple hosts.  I believe that most of the problems that are truly hard are encountered in the single-node scenario, other than the basic problem of building a central Infrastructure graph independently of the individual node graphs, but any complete solution must of course cover how we make sure that the database server is brought up before the web server, in our example.  I believe the central tool will need the ability to trigger host recompiles and runs based on the order of the graph it has built and based on the need to resolve information - i.e., ordering will be based on that central graph, and managed by that central tool.  If a web server needs a database capability, but no host has been allocated and provided that capability, then the web server's catalog cannot be compiled, and a human must be notified to allocate a host for database functionality.  (Or, of course, some kind of automatic allocator must be available.)  One possibility for simplifying the initial version would be to require that all hosts be allocated before the application can be compiled, which would eliminate the possibility of valid configurations with unresolved data and thus make a lot of this much simpler.

On the note of the central infrastructure graph, my entire conception of a central tool is predicated on the assumption that we won't allow cycles in the infrastructure graph, just like we don't in a node graph.  An important thing to keep in mind here is that a cycle in the infrastructure graph can be detected at compile time, and a complete graph, with all dependencies and data resolved, can be compiled before it's ever deployed.  If all needed hosts are allocated, their facts can be used to compile the complete infrastructure graph and each node's graph, and only if that works would you deploy it to production.  As the graph changes over time, any given change needs to result in a viable state.

How data for these capabilities gets resolved is also a critical question.  Some data is obviously manually specified - if you want http instead of https, or postgresql instead of mysql, you need to configure that - but plenty of other data can be resolved with more general strategies, and we will at some point need a general mechanism for managing these strategies.  For instance, every machine knows its IP address (during the compile process), because that's collected as one of the facts, so we need a strategy that knows how to look information for a host up in its facts.  We also can rely heavily on convention for things like database names - in general, the user shouldn't be asked to pick a name for those.  Lastly, things like port numbers and passwords should be able to be generated or dynamically allocated on demand, as appropriate, rather than again asking the user.  It's unclear at this point how those resolution strategies would be tied to the capabilities, but then, the entire process of specifying capabilities is unclear at this point.
